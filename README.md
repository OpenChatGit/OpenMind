# OpenMind

A modern AI chat application built with Electron, React, and local LLM support via Ollama, with HuggingFace cloud inference and local image generation.

## Features

- ğŸ’¬ **Chat with AI models** - Local (Ollama) or Cloud (HuggingFace)
- ğŸ§  **Reasoning support** - View model thinking process (DeepSeek-R1, Qwen-QwQ, etc.)
- ğŸ‘ï¸ **Vision models** - Analyze images with llava, bakllava, moondream
- ğŸ¨ **Image generation** - Local Image-Generation with Diffusers **(Not yet perfect though)**
- ğŸ“ **Image attachments** - File picker or clipboard paste (Ctrl+V)
- ğŸ” **DeepSearch** - Web search with tool use **(Custom could be not perfect)**
- ğŸ”Œ **MCP Tools** - Model Context Protocol support **(Not Really Working now well kinda i gues)**
- ğŸ¤— **HuggingFace** - Cloud inference with HF Pro subscription **(Optional)**
- ğŸ“Š **Inference Stats** - Token counts, speed, duration (like Ollama verbose)
- ğŸ”„ **Regenerate** - Re-run any AI response
- ğŸ“‹ **Copy** - One-click copy AI responses

## Quick Start

```bash
# Install dependencies
npm install

# Run in development
npm run electron

# Build for production
npm run electron:build
```

## Inference Providers

### Local (Ollama)
Run models locally on your machine. Requires [Ollama](https://ollama.ai) installed and running.

```bash
# Install Ollama, then pull a model
ollama pull llama3.2
ollama pull qwen3:4b  # With reasoning support
```

### HuggingFace (Cloud)
Use HuggingFace Inference API for cloud-based models. Requires HF Pro subscription for best models.

1. Get API key from [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)
2. Open Settings â†’ Inference â†’ Select "Hugging Face"
3. Enter your API key

Supported models include Llama, Mistral, Qwen, Phi, and more.

## Image Generation Setup

Image generation runs locally using HuggingFace Diffusers with Python.

### 1. Python Dependencies

```bash
# Option 1: Automatic setup (recommended)
npm run setup:python

# Option 2: Manual install
pip install torch diffusers transformers accelerate safetensors

# Option 3: With CUDA support (NVIDIA GPU - much faster)
pip install torch --index-url https://download.pytorch.org/whl/cu121
pip install diffusers transformers accelerate safetensors
```

### 2. Download a Model

You need to download an image generation model and place it in the `models/` folder.

**Recommended: SDXL-Turbo (fast, good quality)**

1. Go to [huggingface.co/stabilityai/sdxl-turbo](https://huggingface.co/stabilityai/sdxl-turbo)
2. Click "Files and versions"
3. Download the entire folder (or use `git lfs`):
   ```bash
   # Using git (requires git-lfs installed)
   cd models
   git lfs install
   git clone https://huggingface.co/stabilityai/sdxl-turbo
   ```
4. Or download manually: Download all files and put them in `models/sdxl-turbo/`

**Alternative Models:**
- [stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5) - Classic SD 1.5
- [stable-diffusion-2-1](https://huggingface.co/stabilityai/stable-diffusion-2-1) - SD 2.1
- [GGUF quantized models](https://huggingface.co/models?search=stable-diffusion+gguf) - Smaller file size

**Supported formats:**
- **Diffusers** - HuggingFace format (folders with `model_index.json`)
- **GGUF** - Quantized models (requires `pip install stable-diffusion-cpp-python`)
- **Safetensors** - Single file models

### 3. Using Image Generation

1. Click the **ğŸ–¼ï¸ Generate** button in the chat input
2. Select your downloaded model from the dropdown
3. Type a description of the image you want
4. Press Enter

### GPU Acceleration (CUDA)

For NVIDIA GPUs, install PyTorch with CUDA for much faster generation:

```bash
# Windows/Linux
pip install torch --index-url https://download.pytorch.org/whl/cu121

# For GGUF models with GPU:
# Windows (requires Visual Studio Build Tools):
set CMAKE_ARGS=-DSD_CUBLAS=ON
pip install stable-diffusion-cpp-python --force-reinstall --no-cache-dir

# Linux/Mac:
CMAKE_ARGS="-DSD_CUBLAS=ON" pip install stable-diffusion-cpp-python --force-reinstall --no-cache-dir
```

## Message Actions

Hover over any AI response to see action buttons:

- **ğŸ“‹ Copy** - Copy message to clipboard
- **â„¹ï¸ Info** - View inference stats (tokens, speed, duration)
- **ğŸ”„ Regenerate** - Generate a new response for the same prompt

## Reasoning Models

OpenMind supports reasoning/thinking models that show their thought process:

**Ollama:**
- `qwen3:4b`, `qwen3:8b` - Use `/think` or `/no_think` in prompt
- `deepseek-r1:7b`, `deepseek-r1:14b`

**HuggingFace:**
- Models with `<think>` tags or `reasoning_content` field
- DeepSeek-R1, Qwen-QwQ variants

The reasoning is shown in a collapsible "Reasoning" section above the response.

## Tech Stack

- **Frontend:** React 19, Vite
- **Desktop:** Electron
- **AI:** Ollama, HuggingFace Inference API
- **Image Gen:** Python, Diffusers, PyTorch
- **Tools:** MCP SDK

## Project Structure

```
â”œâ”€â”€ electron/          # Electron main process
â”‚   â”œâ”€â”€ main.js        # Main entry, IPC handlers
â”‚   â”œâ”€â”€ huggingface.js # HF API integration
â”‚   â”œâ”€â”€ deepSearch.js  # Web search tools
â”‚   â””â”€â”€ preload.js     # Context bridge
â”œâ”€â”€ src/               # React frontend
â”‚   â”œâ”€â”€ components/    # UI components
â”‚   â””â”€â”€ App.jsx        # Main app
â”œâ”€â”€ python/            # Image generation
â”‚   â””â”€â”€ image_gen.py   # Diffusers script
â””â”€â”€ mcp-tools/         # MCP tool servers
```

## License

MIT
